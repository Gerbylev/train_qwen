{"cells":[{"cell_type":"code","execution_count":null,"id":"5026b682-431a-4e80-baec-62d1158329c3","metadata":{"id":"5026b682-431a-4e80-baec-62d1158329c3"},"outputs":[],"source":["!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git"]},{"cell_type":"code","execution_count":null,"id":"c4b7f648-990c-44c7-b963-243f88a13d5b","metadata":{"id":"c4b7f648-990c-44c7-b963-243f88a13d5b"},"outputs":[],"source":["!pip uninstall -y jax\n","!pip install -e .[bitsandbytes,liger-kernel]\n","!pip install pandas\n","!pip install huggingface_hub"]},{"cell_type":"code","execution_count":null,"id":"7eb41c91-b09a-41fd-94ec-cf9ff5cb24e9","metadata":{"id":"7eb41c91-b09a-41fd-94ec-cf9ff5cb24e9"},"outputs":[],"source":["# Лучше не трогать\n","%cd LLaMA-Factory"]},{"cell_type":"code","execution_count":null,"id":"2e13c5ec-7e2f-44c5-9361-bccd27248bd9","metadata":{"id":"2e13c5ec-7e2f-44c5-9361-bccd27248bd9"},"outputs":[],"source":["!pip install -r requirements.txt\n","!pip install llamafactory"]},{"cell_type":"code","execution_count":null,"id":"d4315598-5204-4a27-ac08-cd0d1020603c","metadata":{"id":"d4315598-5204-4a27-ac08-cd0d1020603c"},"outputs":[],"source":["%cd .."]},{"cell_type":"code","execution_count":null,"id":"160f3bd2-2e03-45ea-bd73-ffc116143dc6","metadata":{"id":"160f3bd2-2e03-45ea-bd73-ffc116143dc6"},"outputs":[],"source":["base_model = \"Qwen/Qwen2.5-32B-Instruct\""]},{"cell_type":"code","execution_count":null,"id":"f50a7fa2-35d8-47d0-8bc5-941714b9dc3e","metadata":{"id":"f50a7fa2-35d8-47d0-8bc5-941714b9dc3e"},"outputs":[],"source":["from huggingface_hub import login, snapshot_download\n","\n","login(\"hf_token\")\n","\n","snapshot_download(base_model)"]},{"cell_type":"code","execution_count":null,"id":"1892c7ca-c558-4231-8bf5-e737a6752e80","metadata":{"id":"1892c7ca-c558-4231-8bf5-e737a6752e80"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"id":"fbc3ce51-e3d1-447b-b074-f721d7009758","metadata":{"id":"fbc3ce51-e3d1-447b-b074-f721d7009758"},"outputs":[],"source":["!mc ls minio/workspace/models/lora/Qwen/Qwen2.5-32B-Instruct/matching/"]},{"cell_type":"code","execution_count":null,"id":"8a5150aa-b956-492f-8518-e68dd2b3e600","metadata":{"id":"8a5150aa-b956-492f-8518-e68dd2b3e600"},"outputs":[],"source":["lora_path = f\"minio/workspace/models/lora/Qwen/Qwen2.5-32B-Instruct/matching/train_2025-01-28-10:30:29\""]},{"cell_type":"code","execution_count":null,"id":"f4f7089a-1473-4a92-bbe6-551dde96729a","metadata":{"id":"f4f7089a-1473-4a92-bbe6-551dde96729a"},"outputs":[],"source":["lora_output_path = f\"/home/jupyter/lora/{'/'.join(lora_path.split('/')[-4:])}\""]},{"cell_type":"code","execution_count":null,"id":"d297d7b3-cbf9-4baa-adb7-135628a379c3","metadata":{"id":"d297d7b3-cbf9-4baa-adb7-135628a379c3"},"outputs":[],"source":["lora_output_path"]},{"cell_type":"code","execution_count":null,"id":"70c10cd1-c687-43c8-a32e-891372a2942a","metadata":{"id":"70c10cd1-c687-43c8-a32e-891372a2942a"},"outputs":[],"source":["!ls  /home/jupyter/lora/Qwen/Qwen2.5-32B-Instruct/matching/train_2025-01-28-10:30:29"]},{"cell_type":"code","execution_count":null,"id":"9ad98329-ae7c-4d6c-82c6-076a7408f8a4","metadata":{"id":"9ad98329-ae7c-4d6c-82c6-076a7408f8a4"},"outputs":[],"source":["command = f\"\"\"\n","mc cp \\\n","{lora_path} \\\n","{'/'.join(lora_output_path.split('/')[:-1])} --recursive\n","\"\"\"\n","\n","print(command)\n","\n","get_ipython().system(command)"]},{"cell_type":"code","execution_count":null,"id":"ce0a12e1-9359-410c-bf17-6fd021cb14f9","metadata":{"id":"ce0a12e1-9359-410c-bf17-6fd021cb14f9"},"outputs":[],"source":["command = f\"\"\"\n","\n","llamafactory-cli export \\\n","  --model_name_or_path {base_model} \\\n","  --adapter_name_or_path {lora_output_path} \\\n","  --template qwen \\\n","  --finetuning_type lora \\\n","  --export_dir /home/jupyter/merge_model \\\n","  --export_size 2 \\\n","  --export_device cpu \\\n","  --export_legacy_format false\n","\"\"\"\n","\n","print(command)\n","\n","get_ipython().system(command)\n"]},{"cell_type":"code","execution_count":null,"id":"7416e97e-54a1-438d-8e7f-0608ce91ad91","metadata":{"id":"7416e97e-54a1-438d-8e7f-0608ce91ad91"},"outputs":[],"source":["!rm -Rf AutoFP8\n","!git clone --depth 1 https://github.com/neuralmagic/AutoFP8.git\n","!pip install -e AutoFP8"]},{"cell_type":"code","execution_count":null,"id":"a8e3b663-4c7d-457f-8cc4-30baade5d52d","metadata":{"id":"a8e3b663-4c7d-457f-8cc4-30baade5d52d"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","from auto_fp8 import AutoFP8ForCausalLM, BaseQuantizeConfig\n","\n","pretrained_model_dir = \"/home/jupyter/merge_model\"\n","quantized_model_dir = \"/home/jupyter/merge_model_fp8\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Load and tokenize 512 dataset samples for calibration of activation scales\n","ds = load_dataset(\"mgoin/ultrachat_2k\", split=\"train_sft\").select(range(512))\n","examples = [tokenizer.apply_chat_template(batch[\"messages\"], tokenize=False) for batch in ds]\n","examples = tokenizer(examples, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n","\n","# Define quantization config with static activation scales\n","quantize_config = BaseQuantizeConfig(quant_method=\"fp8\", activation_scheme=\"static\")\n","\n","# Load the model, quantize, and save checkpoint\n","model = AutoFP8ForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n","model.quantize(examples)\n","model.save_quantized(quantized_model_dir)\n"]},{"cell_type":"code","execution_count":null,"id":"48a7d5db-5113-46df-8f90-96e8d3ea1a8b","metadata":{"id":"48a7d5db-5113-46df-8f90-96e8d3ea1a8b"},"outputs":[],"source":["import pandas as pd\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from auto_fp8 import AutoFP8ForCausalLM, BaseQuantizeConfig\n","\n","pretrained_model_dir = \"/home/jupyter/merge_model\"\n","quantized_model_dir = \"/home/jupyter/merge_model_fp8\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","df = pd.read_csv(\"train_data.csv\").head(2000)\n","\n","def format_example(row):\n","    return f\"User: {row['input']}\\nAssistant: {row['output']}\"\n","\n","examples_texts = df.apply(format_example, axis=1).tolist()\n","\n","ds = Dataset.from_dict({\"text\": examples_texts})\n","\n","examples = tokenizer(ds[\"text\"], padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n","\n","quantize_config = BaseQuantizeConfig(quant_method=\"fp8\", activation_scheme=\"static\")\n","\n","model = AutoFP8ForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n","model.quantize(examples)\n","model.save_quantized(quantized_model_dir)\n"]},{"cell_type":"code","execution_count":null,"id":"67eb2703-34b4-4cf4-9c46-f7955e6282e3","metadata":{"id":"67eb2703-34b4-4cf4-9c46-f7955e6282e3"},"outputs":[],"source":["command = f\"\"\"\n","mc cp {quantized_model_dir} minio/workspace/models/merged_fp8/{'/'.join(lora_path.split('/')[-4:])} --recursive\n","\"\"\"\n","print(command)\n","get_ipython().system(command)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}